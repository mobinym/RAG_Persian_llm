import re
from docx import Document
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import requests
from datetime import datetime

# --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ÙØ§ÛŒÙ„ ÙˆØ±Ø¯ ---
def extract_text_from_docx(file_path):
    doc = Document(file_path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip() != ""])
    return text

# --- Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ---
def clean_persian_text(text):
    text = re.sub(r'[\u200c\u200d\u200e\u200f]', '', text)  # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¦ÛŒ
    text = re.sub(r'\s+', ' ', text)  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    return text.strip()

# --- ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú© ---
def split_text(text, max_length=500):
    sentences = re.split(r'(?<=[.!ØŸ])\s+', text)  # Ø¬Ø¯Ø§Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù‚Ø·Ù‡ ÛŒØ§ Ø¹Ù„Ø§Ù…Øª Ø³ÙˆØ§Ù„
    chunks = []
    current = ""
    for s in sentences:
        if len(current) + len(s) < max_length:
            current += s + " "
        else:
            chunks.append(current.strip())
            current = s + " "
    if current:
        chunks.append(current.strip())
    return chunks

# --- Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Faiss ---
def build_faiss_index(chunks, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    return model, index, chunks

# --- Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· ---
def ask_question(question, model, index, chunks, top_k=5):
    q_emb = model.encode([question])
    distances, indices = index.search(np.array(q_emb), top_k)
    retrieved = [chunks[i] for i in indices[0]]
    return "\n---\n".join(retrieved)

# --- Ø§Ø±Ø³Ø§Ù„ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ù‡ Ù…Ø¯Ù„ LLM Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø®ØªÚ¯ÛŒØ±Ø§Ù†Ù‡ ---
def generate_answer(query, context, model_name="gemma3"):
    prompt = f"""
ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªÙ† Ø²ÛŒØ± Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡. Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ Ø¯Ø± Ù…ØªÙ† Ù†ÛŒØ³ØªØŒ Ø¨Ù†ÙˆÛŒØ³: Â«Ø¯Ø± Ù…ØªÙ† Ø§Ø´Ø§Ø±Ù‡â€ŒØ§ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.Â»
Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ùˆ Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…ØªÙ† Ù†Ú¯Ùˆ.

--- Ù…ØªÙ† ---
{context}
-------------

Ø³Ø¤Ø§Ù„: {query}

Ù¾Ø§Ø³Ø® (ÙÙ‚Ø· Ø§Ø² Ù…ØªÙ† Ø¨Ø§Ù„Ø§):
"""
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": model_name, "prompt": prompt, "stream": False}
    )
    return response.json()["response"]

# --- Ø«Ø¨Øª Ù„Ø§Ú¯ Ú©Ø§Ù…Ù„ ---
def log_interaction(question, context, answer, log_file="rag_log.txt"):
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"\n\n====== {datetime.now()} ======\n")
        f.write(f"â“ Ø³Ø¤Ø§Ù„:\n{question}\n")
        f.write(f"ğŸ“š Ù…ØªÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡:\n{context}\n")
        f.write(f"âœ… Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„:\n{answer}\n")
        f.write("====================================\n")

# --- Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù…Ø¬Ø§Ø² ---
def check_answer_validity(answer, context, blacklist_words=None):
    if blacklist_words is None:
        blacklist_words = ["Ù¾Ù„ÛŒØ³", "Ø®Ø§Ù†Ù‡", "Ù…Ø¨Ø§Ø±Ø²Ù‡", "Ø±Ù‚ÛŒØ¨", "Ù…Ø§Ø´ÛŒÙ†", "Ø¢Ù…Ø§Ø¯Ù‡", "Ø®ÙˆÙ†Ù‡", "Ø¨ÛŒØ±ÙˆÙ†", "Ø®Ø§Ø±Ø¬", "Ù…Ø§Ø´ÛŒÙ† Ù¾Ù„ÛŒØ³"]
    
    answer_lower = answer.lower()
    context_lower = context.lower()
    
    for word in blacklist_words:
        if word in answer_lower and word not in context_lower:
            return False, word
    return True, None

# --- Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® ---
if __name__ == "__main__":
    file_path = r"C:\Users\m.yaghoubi\Desktop\rag\adamiyan_[www.ketabesabz.com].docx"  # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ docx Ø®ÙˆØ¯Øª

    raw_text = extract_text_from_docx(file_path)
    clean_text = clean_persian_text(raw_text)
    chunks = split_text(clean_text)
    model, index, chunks = build_faiss_index(chunks)

    while True:
        user_question = input("\nğŸ”¹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø¨Ù†ÙˆÛŒØ³ exit):\n")
        if user_question.strip().lower() == "exit":
            print("Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            break

        context = ask_question(user_question, model, index, chunks)
        answer = generate_answer(user_question, context)

        valid, bad_word = check_answer_validity(answer, context)
        retry_count = 0
        max_retries = 3

        while not valid and retry_count < max_retries:
            print(f"\nâš ï¸ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„ Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ù‡ ØºÛŒØ±Ù…Ø¬Ø§Ø² '{bad_word}' Ø§Ø³Øª. Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…...")
            prompt = f"""
ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªÙ† Ø²ÛŒØ± Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡. Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ Ø¯Ø± Ù…ØªÙ† Ù†ÛŒØ³ØªØŒ Ø¨Ù†ÙˆÛŒØ³: Â«Ø¯Ø± Ù…ØªÙ† Ø§Ø´Ø§Ø±Ù‡â€ŒØ§ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.Â»
Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ùˆ Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…ØªÙ† Ù†Ú¯Ùˆ. Ú©Ù„Ù…Ù‡ '{bad_word}' Ø±Ø§ Ø¯Ø± Ø¬ÙˆØ§Ø¨ Ù†ÛŒØ§ÙˆØ±.

--- Ù…ØªÙ† ---
{context}
-------------

Ø³Ø¤Ø§Ù„: {user_question}

Ù¾Ø§Ø³Ø® (ÙÙ‚Ø· Ø§Ø² Ù…ØªÙ† Ø¨Ø§Ù„Ø§):
"""
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": "gemma3", "prompt": prompt, "stream": False}
            )
            answer = response.json()["response"]
            valid, bad_word = check_answer_validity(answer, context)
            retry_count += 1

        if not valid:
            answer = "âš ï¸ Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ÛŒ Ø¯Ø± Ù…ØªÙ† Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ ÛŒØ§ Ù…Ø¯Ù„ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ù†ÛŒØ³Øª."

        print("\nâœ… Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ:\n", answer)
        log_interaction(user_question, context, answer)
